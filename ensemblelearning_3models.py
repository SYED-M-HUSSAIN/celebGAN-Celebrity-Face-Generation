# -*- coding: utf-8 -*-
"""EnsembleLearning_3Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KyoUh2TRaFrj5Jzb-PPYoYhtrMx1wXk3
"""

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import Activation,Dense,Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os, json, math, librosa
import IPython.display as ipd
import librosa.display
import tensorflow as tf
import random
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint
from google.colab import drive
from statistics import mode
from google.colab import files
import time
import matplotlib.pyplot as plt
import numpy as np
import pathlib
drive.mount('/content/drive')

#dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"

#data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)

data_dir = pathlib.Path('/content/drive/MyDrive/2_disease')

# Seed environment
seed_value = 1 # seed value

# Set`PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# Set the `python` built-in pseudo-random generator at a fixed value
import random
random.seed = seed_value

# Set the `numpy` pseudo-random generator at a fixed value
import numpy as np
np.random.seed = seed_value

# Set the `tensorflow` pseudo-random generator at a fixed value import tensorflow as tf tf.seed = seed_value

# set configs
base_path = data_dir
target_size = (224,224,3)

# define shape for all images # get classes
classes = os.listdir(base_path)
print(classes)

from keras.preprocessing.image import ImageDataGenerator

batch_size = 32

datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=20,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   vertical_flip = True,
                                   validation_split=0.25)
train_gen = datagen.flow_from_directory(base_path,
                                               target_size=target_size[:2],
                                               batch_size=batch_size,
                                               class_mode='categorical',
                                               subset='training')
val_gen =  datagen.flow_from_directory(base_path,
                                               target_size=target_size[:2],
                                               batch_size=batch_size,
                                               class_mode='categorical',
                                               subset='validation',
                                               shuffle=False)

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dropout, Flatten, Dense
from tensorflow.keras.models import Model
# Import the VGG16 pretrained model
from tensorflow.keras.applications import VGG16

# initialize the model
vgg16 = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)

# Freeze all but the last 3 layers
for layer in vgg16.layers[:-3]: layer.trainable = False

# build model
input = vgg16.layers[-1].output # input is the last output from vgg16

x = Dropout(0.25)(input)
x = Flatten()(x)
output = Dense(2, activation='softmax')(x)

# create the model
vgg16_model = Model(vgg16.input, output, name='VGG16_Model')
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
vgg16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)

early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)

checkpoint = ModelCheckpoint('CustomModel.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 10

# train model
vgg16_model.fit(train_gen, validation_data=val_gen,  epochs= epochs, callbacks= callbacks)

# Evaluate the model
vgg16_model.evaluate(val_gen)

from sklearn.metrics import classification_report

# Evaluate the model
val_pred = vgg16_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels,target_names=classes)
print(classification_rep)

# function to plot confusion matrix

import itertools

def plot_confusion_matrix(actual, predicted):

    cm = confusion_matrix(actual, predicted)
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(3,3))
    cmap=plt.cm.Blues
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title('Confusion matrix', fontsize=12)

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90, fontsize=7)
    plt.yticks(tick_marks, classes, fontsize=7)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], '.2f'),
        horizontalalignment="center",
        color="white" if cm[i, j] > thresh else "black", fontsize = 7)

    plt.ylabel('True label', fontsize=10)
    plt.xlabel('Predicted label', fontsize=10)
    plt.show()

# plot confusion matrix
plot_confusion_matrix(val_true_labels, val_pred_labels)

"""#2nd model"""

# Build model
input = Input(shape= target_size)

x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(input)
x = MaxPool2D(2,2)(x)

x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)
x = MaxPool2D(2,2)(x)

x = Conv2D(filters=128, kernel_size=(3,3), activation='relu')(x)
x = MaxPool2D(2,2)(x)

x = Conv2D(filters=256, kernel_size=(3,3), activation='relu')(x)
x = MaxPool2D(2,2)(x)

x = Dropout(0.25)(x)
x = Flatten()(x)
x = Dense(units=128, activation='relu')(x)
x = Dense(units=64, activation='relu')(x)
output = Dense(units=2, activation='softmax')(x)

custom_model  = Model(input, output, name= 'Custom_Model')

# create the model
custom_model = Model(input, output, name='custom_Model')
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
custom_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)

early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)

checkpoint = ModelCheckpoint('CustomModel.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 10

# train model
custom_model.fit(train_gen, validation_data=val_gen,  epochs= epochs, callbacks= callbacks)
# Evaluate the model
custom_model.evaluate(val_gen)
from sklearn.metrics import classification_report

# Evaluate the model
val_pred = custom_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels,target_names=classes)
print(classification_rep)

# plot confusion matrix
plot_confusion_matrix(val_true_labels, val_pred_labels)

"""#CONCATENATE MODELS"""

# concatenate the models

# import concatenate layer
from tensorflow.keras.layers import Concatenate

# get list of models
models = [custom_model, vgg16_model]

input = Input(shape=(224, 224, 3), name='input') # input layer

# get output for each model input
outputs = [model(input) for model in models]

# contenate the ouputs
x = Concatenate()(outputs)

# add further layers
x = Dropout(0.5)(x)
output = Dense(2, activation='softmax', name='output')(x) # output layer

# create concatenated model
conc_model = Model(input, output, name= 'Concatenated_Model')

# show model structure
from tensorflow.keras.utils import plot_model
plot_model(conc_model)

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
conc_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)

early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)

checkpoint = ModelCheckpoint('Concatenated_Model.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 10

# train model
conc_model.fit(train_gen, validation_data=val_gen,  epochs= epochs, callbacks= callbacks)
# Evaluate the model
conc_model.evaluate(val_gen)
from sklearn.metrics import classification_report

# Evaluate the model
val_pred = conc_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels,target_names=classes)
print(classification_rep)

# plot confusion matrix
plot_confusion_matrix(val_true_labels, val_pred_labels)